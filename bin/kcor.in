#!/usr/bin/env python

import argparse
import codecs
import datetime
from email.mime.text import MIMEText
import fnmatch
import glob
import json
import logging
import math
import os
import pathlib
import psutil
import re
import shutil
import smtplib
import socket
import subprocess
import sys
import time


PY3 = sys.version_info[0] == 3

if PY3:
    import configparser
else:
    import ConfigParser as configparser

try:
    from astropy.io import fits
    from astropy.utils.exceptions import AstropyUserWarning
    import warnings

    warnings.simplefilter("ignore", category=AstropyUserWarning)
    ls_requirements = True
except ModuleNotFoundError as e:
    ls_requirements = False

try:
    import mysql
    import mysql.connector
    import packaging.specifiers
    import packaging.version

    versions_requirements = True
except ModuleNotFoundError as e:
    versions_requirements = False

try:
    import rich

    rich_text = True
except ModuleNotFoundError as e:
    rich_text = False


# setup the logging mechanism
logging.basicConfig(
    format="%(asctime)s %(message)s", datefmt="%Y-%m-%d %H:%M:%S", level=logging.DEBUG
)

# set to True to not launch real scripts
TEST = False

DEVNULL = open(os.devnull, "w")
PIPELINE_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

POLL_SECS = 0.5

LEVELS = ["DEBUG", "INFO", "WARN", "ERROR", "CRITICAL"]
LOG_DIR = "/hao/acos/kcor/logs"

CONFIG_DIR = "@CONFIG_DIR@"

intervals = (
    ("weeks", 604800),  # 60 * 60 * 24 * 7
    ("days", 86400),  # 60 * 60 * 24
    ("hrs", 3600),  # 60 * 60
    ("mins", 60),
    ("secs", 1),
)


def get_config_filename(flags):
    config_basename = f"kcor.{flags}.cfg"

    # construct config file filename
    config_filename = os.path.join(CONFIG_DIR, config_basename)

    return config_filename


def get_config(flags, error):
    config_filename = get_config_filename(flags)
    if not os.path.isfile(config_filename):
        config_basename = os.path.basename(config_filename)
        error(f"configuration file does not exist: {config_basename}")

    # read config file to get arguments to launch data/processing simulators
    config = configparser.ConfigParser()
    config.read(config_filename)

    return config


def display_time(seconds, granularity=2):
    result = []

    for name, count in intervals:
        value = seconds // count
        if value:
            seconds -= value * count
            if value == 1:
                name = name.rstrip("s")
            result.append("%d %s" % (value, name))
    return " ".join(result[:granularity])


def print_bold(text):
    if rich_text:
        rich.print("[bold magenta]" + text + "[/]")
    else:
        print(text)


def format_timedelta(timedelta):
    return display_time(int(timedelta.total_seconds()), granularity=len(intervals))


def convert_boolean(value):
    return True if value.lower() in {"1", "yes", "true"} else False


def notify_completed(args, task):
    config_filename = get_config_filename(args.flags)

    # read config file to get arguments to launch data/processing simulators
    config = configparser.ConfigParser()
    config.read(config_filename)

    try:
        send_notification = convert_boolean(config.get("notifications", "send"))
    except (configparser.NoSectionError, configparser.NoOptionError) as e:
        send_notification = False

    try:
        notification_email = config.get("notifications", "email")
    except (configparser.NoSectionError, configparser.NoOptionError) as e:
        send_notification = False

    if send_notification:
        userhome = os.path.expanduser("~")
        user = os.path.split(userhome)[-1]
        hostname = socket.gethostname()

        dates = ",".join(args.dates)

        with open(config_filename, "r") as f:
            text = f.read()

        msg = MIMEText(text)
        msg["Subject"] = (
            f"KCor {task} ({args.flags}) job completed for {dates} on {hostname}"
        )
        msg["From"] = "kcor-pipeline@ucar.edu"
        msg["To"] = notification_email

        try:
            s = smtplib.SMTP("localhost")
            s.send_message(msg)
            s.quit()
        except ConnectionRefusedError as e:
            print(e)
            print(f"Problem sending email: {msg['Subject']}")


def increment_date(date):
    format = "%Y%m%d"
    d = datetime.datetime.strptime(date, format)
    d += datetime.timedelta(days=1)
    return d.strftime(format)


def split_date_range(date_expr, error):
    date_range_re = re.compile("^[12][0-9]{7}-[12][0-9]{7}$")
    if date_range_re.match(date_expr):
        start_date = date_expr[0:8]
        end_date = date_expr[9:17]
        if end_date <= start_date:
            error(f"end of range before start of range: {date_expr}")
        return start_date, end_date
    error(f"not a range: {date_expr}")


def split_dates(date_expr, error):
    dates = []
    date_re = re.compile("^[12][0-9]{7}$")
    date_range_re = re.compile("^[12][0-9]{7}-[12][0-9]{7}$")
    for d in date_expr.split(","):
        if date_re.match(d):
            dates.append(d)
        elif date_range_re.match(d):
            start_date = d[0:8]
            end_date = d[9:17]
            if end_date <= start_date:
                error(f"end of range before start of range: {d}")
            date = start_date
            while date < end_date:
                dates.append(date)
                date = increment_date(date)
        else:
            error(f"invalid date expression: {d}")

    return dates


def get_login(flags, error):
    config_filename = get_config_filename(flags)
    if not os.path.isfile(config_filename):
        basename = os.path.basename(config_filename)
        error(f"configuration file does not exist: {basename}")

    kcor_config = configparser.ConfigParser()
    kcor_config.read(config_filename)

    try:
        mysql_config_filename = kcor_config.get("database", "config_filename")
        mysql_config_section = kcor_config.get("database", "config_section")
    except configparser.NoSectionError:
        args.parser.error("database information not specified")

    mysql_config = configparser.ConfigParser()
    mysql_config.read(mysql_config_filename)

    try:
        host = mysql_config.get(mysql_config_section, "host")
        user = mysql_config.get(mysql_config_section, "user")
        password = mysql_config.get(mysql_config_section, "password")
        port = mysql_config.get(mysql_config_section, "port")
        database = mysql_config.get(mysql_config_section, "database")
    except configparser.NoSectionError:
        args.parser.error("incomplete database information")

    return (host, user, password, port, database)


# list sub-command
def list_processes(args):
    kcor_processes = []
    for p in psutil.process_iter():
        cmdline = p.cmdline()
        cmdline = "" if len(cmdline) == 0 else cmdline[-1]
        if p.name() == "idl" and cmdline.startswith("kcor"):
            kcor_processes.append(
                {"cmdline": cmdline, "pid": p.pid, "start_time": p.create_time()}
            )
    if len(kcor_processes) == 0:
        print("no kcor processes currently running")
        return

    now = datetime.datetime.now()

    for p in kcor_processes:
        started = datetime.datetime.fromtimestamp(p["start_time"])
        time_running = format_timedelta(now - started)
        start_time = started.strftime("%Y-%m-%d %H:%M:%S")
        pid = p["pid"]
        cmdline = p["cmdline"]
        print(f"[{pid}] ({start_time} running {time_running}): {cmdline}")


def get_raw_basedir(date, flags, error):
    ucomp_config = get_config(flags, error)

    try:
        raw_basedir = ucomp_config.get("processing", "raw_basedir")
    except configparser.NoSectionError:
        error("no raw section in config file")
    except configparser.NoOptionError:
        routing_filename = ucomp_config.get("processing", "routing_file")
        routing_config = configparser.ConfigParser()
        routing_config.read(routing_filename)
        date_locations = routing_config.options("kcor")
        for date_expr in date_locations:
            if fnmatch.fnmatch(date, date_expr):
                return routing_config.get("kcor", date_expr)
    return raw_basedir


def has_raw_data(date, flags, error):
    raw_basedir = get_raw_basedir(date, flags, error)
    raw_dir = os.path.join(raw_basedir, date)

    toplevel_fits_glob = os.path.join(raw_dir, "*.fts*")
    n_toplevel_fits_files = len(glob.glob(toplevel_fits_glob))

    level0_fits_glob = os.path.join(raw_dir, "level0", "*.fts*")
    n_level0_fits_files = len(glob.glob(level0_fits_glob))

    return n_toplevel_fits_files + n_level0_fits_files > 0


def raw_locked(date, flags, error):
    raw_basedir = get_raw_basedir(date, flags, error)
    raw_dir = os.path.join(raw_basedir, date)
    lock_filename = os.path.join(raw_dir, ".lock")
    return os.path.exists(lock_filename)


# versions sub-command
def versions(args):
    if not versions_requirements:
        args.parser.error("missing Python packages required for querying database")

    dates = split_dates(",".join(args.dates), args.parser.error)
    host, user, password, port, database = get_login(args.flags, args.parser.error)

    versions_list = []
    try:
        connection = mysql.connector.connect(host=host, user=user, password=password)
        cursor = connection.cursor()
        for d in dates:
            versions_list.append(get_version(d, cursor))
    except mysql.connector.Error as e:
        print(e)
    finally:
        cursor.close()
        connection.close()

    filter = make_version_filter(args.filter)

    if args.oldest:
        f = [v for v in versions_list if v[1] is not None and v[1][0] != "-"]
        s = sorted(f, key=lambda v: packaging.version.parse(v[1]))
        for d, v, r in s:
            if v != "<unknown>":
                print(f"{d}: {v:12s} {r:8s}")
                break
    if args.none:
        f = [v for v in versions_list if v[1] is not None and v[1][0] != "-"]
        no_version = [v for v in versions_list if v[1] is None or v[1][0] == "-"]
        in_progress = [
            v for v in no_version if has_raw_data(v[0], args.flags, args.parser.error)
        ]
        for d, v, r in in_progress:
            attribute = "L" if raw_locked(d, args.flags, args.parser.error) else "+"
            v = v if v is not None else 12 * "-"
            r = r if r is not None else 12 * "-"
            print(f"{d}: {attribute} {v:12s} {r:8s}")
    elif args.summary:
        f = [v for v in versions_list if v[1] is not None and v[1][0] != "-"]
        s = sorted(f, key=lambda v: packaging.version.parse(v[1]))
        current_version = ""
        n_dates = 0
        for d, v, r in s:
            if v == current_version:
                n_dates += 1
            else:
                if n_dates != 0 and filter(current_version):
                    plural = "s" if n_dates > 1 else ""
                    print(f"{current_version}: {n_dates} day{plural}")
                current_version = v
                n_dates = 1
        else:
            if n_dates > 0 and filter(current_version):
                plural = "s" if n_dates > 1 else ""
                print(f"{current_version}: {n_dates} day{plural}")
    else:
        for d, v, r in versions_list:
            if has_raw_data(d, args.flags, args.parser.error):
                if raw_locked(d, args.flags, args.parser.error):
                    attribute = "L"
                else:
                    attribute = "+"
            else:
                attribute = "-"

            if filter(v):
                v = v if v is not None else 12 * "-"
                r = r if r is not None else 12 * "-"
                print(f"{d}: {attribute} {v:12s} {r:8s}")


# TODO: estimate parser
# $ kcor estimate --filter ">=2.2.0" --start-date 2025-08-29T20:02:48
# 2025-09-15T08:10:30
def estimate(args):
    if not versions_requirements:
        args.parser.error("missing Python packages required for querying database")

    host, user, password, port, database = get_login(args.flags, args.parser.error)

    try:
        connection = mysql.connector.connect(host=host, user=user, password=password)
        cursor = connection.cursor()
        q = f"""select sw.sw_version, MLSO.mlso_numfiles.obs_day from
(select MLSO.kcor_sw.sw_version, MLSO.kcor_eng.obs_day from MLSO.kcor_eng inner join MLSO.kcor_sw on MLSO.kcor_sw.sw_id=MLSO.kcor_eng.kcor_sw_id group by MLSO.kcor_eng.obs_day) as sw
inner join MLSO.mlso_numfiles on MLSO.mlso_numfiles.day_id=sw.obs_day;"""
        cursor.execute(q)
        versions_list = cursor.fetchall()
    except mysql.connector.Error as e:
        print(type(e))
        print(e)
    finally:
        cursor.close()
        connection.close()

    filter = make_version_filter(args.filter)
    n_total_dates = len(versions_list)
    n_ok_dates = 0
    for v, d in versions_list:
        if filter(v):
            n_ok_dates += 1

    date_fmt = "%Y-%m-%dT%H:%M:%S"
    start_date = datetime.datetime.strptime(args.start_date, date_fmt)
    now = datetime.datetime.now()
    end_date = start_date + n_total_dates * (now - start_date) / n_ok_dates
    percentage_complete = 100.0 * n_ok_dates / n_total_dates
    print(f"{percentage_complete:0.1f}% complete")
    print(datetime.datetime.strftime(end_date, date_fmt))


def make_version_filter(filter_expression):
    def null_filter(version):
        return True

    if filter_expression is None:
        return null_filter

    spec = packaging.specifiers.SpecifierSet(filter_expression, prereleases=True)

    def filter_function(version):
        return packaging.version.parse(version) in spec

    return filter_function


def get_version(date, cursor):
    null_version = "----------"
    null_revision = "--------"
    unknown_version = "<unknown>"
    unknown_revision = "<unknown>"

    year = date[0:4]
    month = date[4:6]
    day = date[6:8]
    q = f"select * from MLSO.mlso_numfiles where obs_day='{year}-{month}-{day}';"
    cursor.execute(q)
    row = cursor.fetchone()
    if row is None:
        return (date, None, None)
    else:
        day_id = row[0]

        q = f"select kcor_sw_id from MLSO.kcor_eng where obs_day={day_id} limit 1;"
        cursor.execute(q)
        row = cursor.fetchone()
        if row is None:
            return (date, null_version, null_revision)
        else:
            sw_id = row[0]
            if sw_id is None:
                return (date, unknown_version, unknown_revision)

            q = f"select * from MLSO.kcor_sw where sw_id={sw_id};"
            cursor.execute(q)
            row = cursor.fetchone()
            if row is None:
                return (date, null_version, null_revision)
            else:
                return (date, row[2], row[3])


# report sub-command
def report(args):
    if not versions_requirements:
        args.parser.error("missing Python packages required for querying database")

    dates = split_dates(",".join(args.dates), args.parser.error)
    host, user, password, port, database = get_login(args.flags, args.parser.error)

    try:
        connection = mysql.connector.connect(host=host, user=user, password=password)
        cursor = connection.cursor()
        first_line = True
        for d in dates:
            if not first_line and args.verbose:
                print()
            get_report(d, cursor, verbose=args.verbose)
            first_line = False
    except mysql.connector.Error as e:
        print(e)
    finally:
        cursor.close()
        connection.close()


def get_producttype(name: str, cursor):
    q = f'select producttype_id from MLSO.mlso_producttype where producttype="{name}"'
    cursor.execute(q)
    row = cursor.fetchone()
    return row[0]


def get_obsday(date: str, cursor):
    q = f'select day_id from MLSO.mlso_numfiles where obs_day="{date[0:4]}-{date[4:6]}-{date[6:8]}";'
    cursor.execute(q)
    row = cursor.fetchone()
    if row is None:
        return None
    return row[0]


def get_count_by_producttype(producttype_id, obsday_id, cursor):
    q = f"select count(*) from MLSO.kcor_img where obs_day={obsday_id} and producttype={producttype_id};"
    cursor.execute(q)
    row = cursor.fetchone()
    return row[0]


def get_pb_timerange(obsday_id, cursor):
    producttype_id = get_producttype("pB", cursor)
    q = f"select date_obs from MLSO.kcor_img where obs_day={obsday_id} and producttype={producttype_id} order by date_obs;"
    cursor.execute(q)
    rows = cursor.fetchall()
    if len(rows) == 0:
        return None
    first_dt = rows[0][0]
    last_dt = rows[-1][0]
    time_format = "%H:%M:%S"
    return (first_dt.strftime(time_format), last_dt.strftime(time_format))


def get_report(date, cursor, verbose=False):
    obsday_id = get_obsday(date, cursor)
    if obsday_id is None:
        print(f"{date} --- no data --- ")
        return

    time_range = get_pb_timerange(obsday_id, cursor)
    if time_range is None:
        print(f"{date} --- no data ---")
        return

    if verbose:
        print(f"{date} [{time_range[0]} - {time_range[1]}]")

        indent = 2 * " "
        gap = 6 * " "
        types = [["pB", "pbavg", "pbextavg"], ["nrgf", "nrgfavg", "nrgfextavg"]]
        max_width = max([len(t) for row in types for t in row])
        for row in types:
            producttype_ids = [get_producttype(t, cursor) for t in row]
            counts = [
                get_count_by_producttype(id, obsday_id, cursor)
                for id in producttype_ids
            ]
            labels = [f"{c} {'files' if c != 1 else 'file'}" for c in counts]
            s = gap.join(
                [
                    f"{indent}{t:{max_width}s} : {label:10s}"
                    for t, label in zip(row, labels)
                ]
            )
            print(s)
    else:
        pb_id = get_producttype("pB", cursor)
        n_pb = get_count_by_producttype(pb_id, obsday_id, cursor)
        units = "pB file" if n_pb == 1 else "pB files"
        n_files = f"{n_pb} {units}"
        print(f"{date} {n_files:13s} [{time_range[0]} - {time_range[1]}]")


# ls sub-command
def file_lines(filename):
    n_lines = 0

    with open(filename, "r") as f:
        for line in f.readlines():
            n_lines += 1
    return n_lines


def get_fits_boolean(header, keyword):
    return header[keyword] == "in" if keyword in header else False


def get_fits_float(header, keyword):
    return header[keyword] if keyword in header else None


def value2str(v):
    if type(v) == str:
        return f"{v:10s}"
    elif type(v) == float:
        return f"{v:8.3f}"
    elif type(v) == int:
        return f"{v:8d}"
    elif type(v) == bool:
        return f"{v!s:5s}"
    elif v is None:
        return 5 * "-"
    return f"{v}"


def list_fits_file(f, columns):
    basename = os.path.basename(f)
    line = f"{basename:36s}"
    with fits.open(f) as fits_file:
        fits_file[0].verify("fix")
        primary_header = fits_file[0].header
        for c in columns:
            v = primary_header[c] if c in primary_header else None
            v = value2str(v)
            line += f"  {v}"
    print(line)


def list_fits_file_default(f):
    basename = os.path.basename(f)
    with fits.open(f) as fits_file:
        fits_file[0].verify("fix")
        header = fits_file[0].header
        diffuser = get_fits_boolean(header, "DIFFUSER")
        calpol = get_fits_boolean(header, "CALPOL")
        darkshutter = get_fits_boolean(header, "DARKSHUT") or get_fits_boolean(
            header, "COVER"
        )
        if darkshutter:
            type = "dark"
        elif calpol:
            calpang = header["CALPANG"] if "CALPANG" in header else 0.0
            type = f"cal {calpang:0.1f}Â°"
        elif diffuser:
            type = "flat"
        else:
            type = "science"

        level = header["LEVEL"]
        occulter = header["OCCLTRID"]
        sgsscint = get_fits_float(header, "SGSSCINT")
        seeing = f"{sgsscint:7.1f} arcsec seeing" if sgsscint is not None else 20 * "-"

    print(f"{basename:40s}  {level:2s}  {type:10s}  {occulter:18s}  {seeing}")


def list_files(files, columns=None):
    for f in files:
        filename, file_extension = os.path.splitext(f)
        basename = os.path.basename(f)
        if os.path.isdir(f):
            n_subfiles = len(glob.glob(os.path.join(f, "*")))
            name = f"{basename}/"
            print(f"{name:40s}  {n_subfiles:,} files")
        elif os.path.isfile(f):
            if file_extension == ".tgz":
                size = os.stat(f).st_size
                print(f"{basename:40s}  {size:,} bytes")
            else:
                try:
                    if columns is None:
                        list_fits_file_default(f)
                    else:
                        list_fits_file(f, columns)
                except OSError:
                    if file_extension in [".log", ".olog", ".txt", ".cfg", ".tarlist"]:
                        n_lines = file_lines(f)
                        s = "s" if n_lines != 1 else ""
                        print(f"{basename:36s}  {n_lines:,} line{s}")
                    else:
                        size = os.stat(f).st_size
                        print(f"{basename:36s}  {size:,} bytes")
        else:
            print(f"{f} - unknown item")


def ls(args):
    if not ls_requirements:
        args.parser.error("missing Python packages required for listing FITS files")

    try:
        files = [f for f in args.files if os.path.isfile(f)]
        dirs = [d for d in args.files if os.path.isdir(d)]

        if len(files) == 0 and len(dirs) == 1:
            items = [f for f in glob.glob(os.path.join(dirs[0], "*"))]
            files = [f for f in items if os.path.isfile(f)]
            dirs = [d for d in items if os.path.isdir(d)]

        if args.keywords is None:
            columns = None
        else:
            columns = args.keywords.split(",")

        list_files(sorted(dirs))
        list_files(sorted(files), columns=columns)
    except KeyboardInterrupt:
        print()


# verify sub-command
def verify(args):
    dates = ",".join(args.dates)

    cmd = [
        os.path.join(PIPELINE_DIR, "bin", "kcor_verify_dates.sh"),
        get_config_filename(args.flags),
        dates,
    ]

    process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
    print("[%d] %s" % (process.pid, " ".join(cmd)))

    terminated = wait_for(process)


# alerts
def list_alerts(args):
    if len(args.json_files) == 0:
        json_files = glob.glob("*.json")
    else:
        json_files = args.json_files

    # sort files by ISSUE_TIME where filenames are of the form:
    #
    #   mlso_kcor.EVENT_TIME.ISSUE_TIME.json
    #
    # where times are of the form YYYY-MM-DDTHHMMSSZ.
    if args.by_issue_time:
        issue_start = 29
        date_length = 18
        json_files = sorted(
            json_files, key=lambda f: f[issue_start : issue_start + date_length]
        )

    for f in json_files:
        basename = os.path.basename(f)
        with open(f) as jf:
            alert = json.load(jf)
        try:
            submission = alert["sep_forecast_submission"]
            observations = submission["observations"][0]
            type = (
                observations["alert"]["alert_type"]
                if "alert" in observations
                else "heartbeat"
            )
            all_clear = observations["all_clear"]["all_clear_boolean"]
            all_clear_text = "clear" if all_clear else "CME"
            pa = (
                submission["triggers"][0]["cme"]["pa"]
                if "triggers" in submission
                else math.nan
            )
            pa_text = f"{pa:0.2f} deg" if math.isfinite(pa) else ""
            print(f"{basename}   {type:12s}  {all_clear_text:5s}   {pa_text}")

            if args.detail:
                json.dump(alert, sys.stdout, indent=4)
                print()
        except KeyError as e:
            print(f"{basename}: invalid JSON alert file")


# log sub-command
def filter_log(args):
    date_prog = re.compile(r"^\d{8}$")
    logfiles = [
        os.path.join(LOG_DIR, f"{f}.{args.type}.log") if date_prog.match(f) else f
        for f in args.logfiles
    ]

    follow = args.follow
    if follow and len(logfiles) > 1:
        print("cannot follow multiple files")
        return

    if args.prune is not None:
        prune_logfiles(logfiles, int(args.prune))
        return

    # default is to not filter
    if args.level:
        level = args.level.upper()
    elif args.critical:
        level = "CRITICAL"
    elif args.error:
        level = "ERROR"
    elif args.warn:
        level = "WARN"
    elif args.info:
        level = "INFO"
    else:
        level = "DEBUG"

    try:
        level_index = LEVELS.index(level)
    except ValueError:
        print(f"invalid level: {level}")
        parser.print_help()
        return

    for i, f in enumerate(logfiles):
        if len(logfiles) > 1:
            if i != 0:
                print("")
            print(f)
            print("-" * len(f))
        filter_file(f, level_index, follow)


def prune_logfiles(files, max_version):
    version_re = re.compile(r"\d+")
    for f in files:
        versions = glob.glob(f"{f}.*")
        for v in versions:
            n = v[len(f) + 1 :]
            if version_re.match(n):
                if int(n) > max_version:
                    file_to_delete = f"{f}.{n}"
                    print(f"rm {file_to_delete}")
                    os.remove(file_to_delete)


def filter_file(logfile, level_index, follow):
    loglevel_filter = "|".join(LEVELS[level_index:])
    loglevel_prog = re.compile(f".*({loglevel_filter}):.*")
    logstart_prog = re.compile(r"(\[\d*\] )?\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}")
    exact_loglevel_prog = re.compile(f".*{LEVELS[level_index]}:.*")

    matched_last_line = False
    exact_matched_last_line = False

    line = "not empty"

    try:
        with codecs.open(logfile, mode="r", encoding="latin-1") as f:
            while follow or line != "":
                try:
                    line = f.readline()
                except UnicodeDecodeError:
                    line = "<UnicodeDecodeError>"
                if line == "":
                    try:
                        time.sleep(POLL_SECS)
                    except IOError:
                        return
                    except KeyboardInterrupt:
                        print()
                        return
                    continue

                if loglevel_prog.match(line):
                    matched_last_line = True
                    exact_matched_last_line = (
                        True if exact_loglevel_prog.match(line) else False
                    )
                    try:
                        if exact_matched_last_line:
                            print(line.rstrip())
                        else:
                            print_bold(line.rstrip())
                    except IOError:
                        return
                    except KeyboardInterrupt:
                        print()
                        return
                else:
                    if matched_last_line:
                        if logstart_prog.match(line):
                            matched_last_line = False
                            exact_matched_last_line = False
                        else:
                            try:
                                if exact_matched_last_line:
                                    print(line.rstrip())
                                else:
                                    print_bold(line.rstrip())
                            except IOError:
                                return
                            except KeyboardInterrupt:
                                print()
                                return
    except IOError:
        print(f"Problem reading {logfile}")


# missing sub-command
def missing(args):
    dates = split_dates(",".join(args.dates), args.parser.error)
    for d in dates:
        raw_dir = get_raw_dir(d, args.flags, args.parser.error)
        raw_datedir = os.path.join(raw_dir, d)
        present_files1 = glob.glob(os.path.join(raw_datedir, "*.fts*"))
        present_files2 = glob.glob(os.path.join(raw_datedir, "level0", "*.fts*"))
        present_files = present_files1 + present_files2
        machine_log = os.path.join(raw_datedir, f"{d}.kcor.machine.log")

        # use t1 log for dates without a machine log
        if not os.path.exists(machine_log):
            machine_log = os.path.join(raw_datedir, f"{d}.kcor.t1.log")

        with open(machine_log, "r") as f:
            created_files = [line.split()[0] for line in f]
        if (
            len(created_files) == 1
            and created_files[0] == os.path.basename(machine_log) + ":"
        ):
            created_files = []

        present_files = [os.path.basename(f) for f in present_files]

        if args.script:
            print("#!/bin/sh")
            config_filename = get_config_filename(args.flags)
            if not os.path.isfile(config_filename):
                basename = os.path.basename(config_filename)
                args.parser.error(f"configuration file does not exist: {basename}")
            kcor_config = configparser.ConfigParser()
            kcor_config.read(config_filename)

            try:
                raw_remote_server = kcor_config.get("verification", "raw_remote_server")
                raw_remote_dir = kcor_config.get("verification", "raw_remote_dir")
                raw_remote_dir = os.path.join(raw_remote_dir, d)
            except configparser.NoSectionError:
                args.parser.error("no verification section in config file")
            except configparser.NoOptionError:
                args.parser.error(
                    "raw remote server options not present in config file"
                )

            print(f"REMOTE_LOCATION={raw_remote_server}:{raw_remote_dir}")
            print(f"LOCAL_LOCATION={raw_datedir}")
            print()
        for cf in created_files:
            if cf not in present_files and cf + ".gz" not in present_files:
                if args.script:
                    print(f"scp $REMOTE_LOCATION/{cf} $LOCAL_LOCATION")
                else:
                    print(cf)


def get_raw_dir(date, flags, error):
    config_filename = get_config_filename(flags)
    if not os.path.isfile(config_filename):
        basename = os.path.basename(config_filename)
        error(f"configuration file does not exist: {basename}")

    kcor_config = configparser.ConfigParser()
    kcor_config.read(config_filename)
    try:
        raw_dir = kcor_config.get("processing", "raw_basedir")
    except configparser.NoSectionError:
        error("no processing section in config file")
    except configparser.NoOptionError:
        routing_filename = kcor_config.get("processing", "routing_file")
        routing_config = configparser.ConfigParser()
        routing_config.read(routing_filename)
        date_locations = routing_config.options("kcor")
        for date_expr in date_locations:
            if fnmatch.fnmatch(date, date_expr):
                return routing_config.get("kcor", date_expr)
    return raw_dir


def wait_for(process):
    try:
        out, err = process.communicate()
        if err != "":
            print(err)
        return 0
    except KeyboardInterrupt:
        print(f"killing process {process.pid}")
        process.kill()
        return 1


class FakeProcess:
    def __init__(self, pid):
        self.pid = pid

    def kill(self):
        pass

    def communicate(self):
        return ("", "")


def launch_process(cmd, **kwargs):
    if TEST:
        process = FakeProcess(0)
    else:
        process = subprocess.Popen(cmd, encoding="utf-8", **kwargs)
    return process


# calibrate (cal) sub-command
def process_cal(args):
    if args.list is not None:
        if len(args.dates) > 1:
            args.parser.error(
                "only a single date allowed when using list for calibration"
            )
        calibrate_list(args.list, args.dates[0], args.flags, args.no_wait, args.parser)
        return

    for d in args.dates:
        calibrate_dates(d, args.flags, args.no_wait, args.parser.error)

    notify_completed(args, "calibration")


def calibrate_list(filelist, dates, flags, no_wait, parser):
    if dates.find(",") >= 0 or dates.find("-") >= 0:
        parser.error("only a single date allowed when using list for calibration")
        return

    cmd = [
        os.path.join(PIPELINE_DIR, "bin", "runkcor_calibrate_list.sh"),
        flags,
        dates,
        filelist,
    ]

    process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
    print("[%d] %s" % (process.pid, " ".join(cmd)))

    if not no_wait:
        terminated = wait_for(process)


def calibrate_dates(dates, flags, no_wait, error):
    for d in dates.split(","):
        if d.find("-") < 0:
            cmd = [
                os.path.join(PIPELINE_DIR, "bin", "runkcor_calibrate.sh"),
                get_config_filename(flags),
                d,
            ]

            process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
            print("[%d] %s" % (process.pid, " ".join(cmd)))

            if not no_wait:
                terminated = wait_for(process)
                if terminated:
                    break
        else:
            start_date, end_date = split_date_range(d, error)
            cmd = [
                os.path.join(PIPELINE_DIR, "bin", "runkcor_calibrate_range.sh"),
                get_config_filename(flags),
                start_date,
                end_date,
            ]

            process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
            print("[%d] %s" % (process.pid, " ".join(cmd)))

            if not no_wait:
                terminated = wait_for(process)
                if terminated:
                    break


# process (both rt and eod) sub-command
def process(args):
    for d in args.dates:
        process_dates(d, "process", args.flags, args.no_wait, args.parser.error)

    notify_completed(args, "process")


# realtime (rt) sub-command
def process_rt(args):
    for d in args.dates:
        process_dates(d, "rt", args.flags, args.no_wait)

    notify_completed(args, "real-time process")


# end-of-day (eod) sub-command
def process_eod(args):
    for d in args.dates:
        process_dates(d, "eod", args.flags, args.no_wait)

    notify_completed(args, "end-of-day process")


# script sub-command
def run_script(args):
    for dates in args.dates:
        for d in dates.split(","):
            if d.find("-") < 0:
                cmd = [
                    os.path.join(PIPELINE_DIR, "bin", f"runkcor_script.sh"),
                    args.name,
                    get_config_filename(args.flags),
                    d,
                ]
                process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
                print("[%d] %s" % (process.pid, " ".join(cmd)))
                if not args.no_wait:
                    terminated = wait_for(process)
                    if terminated:
                        break
            else:
                start_date, end_date = split_date_range(d, args.parser.error)
                cmd = [
                    os.path.join(PIPELINE_DIR, "bin", f"runkcor_script_range.sh"),
                    args.name,
                    get_config_filename(args.flags),
                    start_date,
                    end_date,
                ]

                process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
                print("[%d] %s" % (process.pid, " ".join(cmd)))
                if not args.no_wait:
                    terminated = wait_for(process)
                    if terminated:
                        break

    notify_completed(args, "script")


# CME detection sub-command
def cme_detection(args):
    extra_flags = ["/realtime"] if args.realtime else None
    for d in args.dates:
        process_dates(
            d, "cmedetection", args.flags, args.no_wait, extra_flags=extra_flags
        )

    notify_completed(args, "CME detection")


# savecme sub-command
def savecme(args):
    script = f"savecme"
    for d in args.dates:
        process_dates(d, script, args.flags, args.no_wait)

    notify_completed(args, "save CME results")


# archive sub-command
def archive(args):
    script = f"archive_l{args.level}"
    for d in args.dates:
        process_dates(d, script, args.flags, args.no_wait)


# purge sub-command
def purge(args):
    for d in args.dates:
        process_dates(d, "purge", args.flags, args.no_wait)

    notify_completed(args, "purge")


# remove sub-command
def remove(args):
    for d in args.dates:
        process_dates(d, "remove", args.flags, args.no_wait)

    notify_completed(args, "remove")


def process_dates(dates, script, flags, no_wait, error, extra_flags=None):
    for d in dates.split(","):
        if d.find("-") < 0:
            cmd = [
                os.path.join(PIPELINE_DIR, "bin", f"runkcor_{script}.sh"),
                get_config_filename(flags),
                d,
            ]
            if extra_flags is not None:
                cmd = cmd + extra_flags

            process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
            print("[%d] %s" % (process.pid, " ".join(cmd)))
            if not no_wait:
                terminated = wait_for(process)
                if terminated:
                    break
        else:
            start_date, end_date = split_date_range(d, error)
            cmd = [
                os.path.join(PIPELINE_DIR, "bin", f"runkcor_{script}_range.sh"),
                get_config_filename(flags),
                start_date,
                end_date,
            ]
            if extra_flags is not None:
                cmd = cmd + extra_flags

            process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
            print("[%d] %s" % (process.pid, " ".join(cmd)))
            if not no_wait:
                terminated = wait_for(process)
                if terminated:
                    break


# simulate sub-command
def simulate(args):
    if len(args.dates) > 1:
        args.parser.error("only a single date allowed when using the simulator")

    config_filename = get_config_filename(args.flags)

    # read config file to get arguments to launch data/processing simulators
    config = configparser.ConfigParser()
    config.read(config_filename)

    try:
        depot_basedir = config.get("simulator", "depot_basedir")
    except configparser.NoSectionError:
        args.parser.error("depot_basedir not specified")

    raw_basedir = get_raw_dir(args.dates[0], args.flags, args.parser.error)
    raw_dir = os.path.join(raw_basedir, args.dates[0])

    rt_launch_interval = config.get("simulator", "rt_launch_interval", fallback="60.0")
    eod_launch_interval = config.get(
        "simulator", "eod_launch_interval", fallback="900.0"
    )

    arrival_interval = config.get("simulator", "arrival_interval", fallback="60.0")
    speedup_factor = config.get("simulator", "speedup_factor", fallback="1.0")

    # launch processing simulator
    processing_cmd = [
        os.path.join(PIPELINE_DIR, "bin", "kcor_simulate_processing"),
        "-f",
        args.flags,
        "--rt-frequency",
        rt_launch_interval,
        "--eod-frequency",
        eod_launch_interval,
        args.dates[0],
    ]
    if args.no_eod:
        processing_cmd.insert(2, "--no-eod")
    processing_process = subprocess.Popen(processing_cmd)

    time.sleep(5.0)

    # launch incoming data simulator
    data_cmd = [
        os.path.join(PIPELINE_DIR, "bin", "kcor_simulate_data"),
        "-r",
        raw_dir,
        "-b",
        arrival_interval,
        "-s",
        speedup_factor,
    ]
    time_dir = config.get("simulator", "time_dir")
    if time_dir is not None:
        data_cmd += ["-t", time_dir]
    data_cmd.append(os.path.join(depot_basedir, args.dates[0]))
    data_process = subprocess.Popen(data_cmd)

    if args.cme:
        cmd = [
            os.path.join(PIPELINE_DIR, "bin", f"runkcor_cmedetection.sh"),
            get_config_filename(args.flags),
            args.dates[0],
            "/realtime",
        ]

        cme_process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
        logging.info("launching CME detection process...")

    try:
        while True:
            time.sleep(1.0)
    except KeyboardInterrupt:
        data_pid = data_process.pid
        proc_pid = processing_process.pid
        print(f"killing data ({data_pid}) and processing ({proc_pid}) processes...")
        processing_process.kill()
        data_process.kill()
        if args.cme:
            cme_pid = cme_process.pid
            print(f"killing CME detection ({cme_pid}) process...")
            cme_process.kill()


# stage sub-command
def stage(args):
    dst = os.path.realpath(args.location)
    print(f"staging to {dst}")

    for e in args.dates:
        for r in e.split(","):
            if r.find("-") < 0:
                dates = [r]
            else:
                try:
                    dates = expand_date_range(r)
                except IndexError:
                    args.parser.error(r"bad date range expression: {r}")

            for d in dates:
                stage_date(args.parser, d, args.flags, dst)

    notify_completed(args, "stage")


def expand_date_range(r):
    ends = r.split("-")
    start_dt = datetime.datetime.strptime(ends[0], "%Y%m%d")
    end_dt = datetime.datetime.strptime(ends[1], "%Y%m%d")

    if end_dt < start_dt:
        raise IndexError

    dates = []

    dt = start_dt
    while dt < end_dt:
        dates.append(dt.strftime("%Y%m%d"))
        dt += datetime.timedelta(days=1)

    return dates


def raw_basedir(date, config_filename):
    """Look for processing/raw_basedir first -- if not found, then use
    processing/routing_file.
    """
    config = configparser.ConfigParser()
    config.read(config_filename)

    try:
        raw_basedir = config.get("processing", "raw_basedir")
        return raw_basedir
    except configparser.NoOptionError:
        routing_file = config.get("processing", "routing_file")
        routing = configparser.ConfigParser()
        routing.read(routing_file)
        date_ranges = routing.options("kcor")
        for r in date_ranges:
            if pathlib.PurePath(date).match(r):
                raw_basedir = routing.get("kcor", r)
                return raw_basedir
        return None


def stage_date(parser, date, flags, dst):
    config_filename = get_config_filename(flags)
    src = raw_basedir(date, config_filename)

    if src is None:
        parser.error("raw basedir not specified")

    date_dir = os.path.join(src, date)
    if os.path.isdir(date_dir):
        try:
            copy_date(date, src, dst)
        except FileNotFoundError:
            print(f"files not found for {date} in {src}")
            # parser.error(f"files not found in {src}")
    else:
        parser.error(f"{date_dir} does not exist")


def copy_date(date, src, dst):
    copy_files(date, src, "*.log", dst, verbose=True)
    copy_files(date, src, "*.fts.gz", dst, verbose=True)


def copy_files(date, src, spec, dst, *, verbose=False):
    files = glob.glob(os.path.join(src, date, "level0", spec))
    if not files:
        files = glob.glob(os.path.join(src, date, spec))
        if not files:
            raise FileNotFoundError

    dst_dir = os.path.join(dst, date)
    if not os.path.isdir(dst_dir):
        os.mkdir(dst_dir)

    if verbose:
        print(f"staging {date} [{src}] ({len(files)} {spec} files)...")

    for f in files:
        shutil.copy2(f, dst_dir)


# locate sub-command
def locate(args):
    # get config file
    config_filename = get_config_filename(args.flags)

    if args.dates:
        for e in args.dates:
            for r in e.split(","):
                if r.find("-") < 0:
                    dates = [r]
                else:
                    try:
                        dates = expand_date_range(r)
                    except IndexError:
                        args.parser.error(r"bad date range expression: {r}")

                for d in dates:
                    # get raw_basedir
                    raw_bdir = raw_basedir(d, config_filename)
                    print(f"{raw_bdir}/{d}")
    else:
        config = configparser.ConfigParser()
        config.read(config_filename)
        try:
            raw_bdir = config.get("processing", "raw_basedir")
            print(raw_bdir)
        except configparser.NoOptionError:
            routing_file = config.get("processing", "routing_file")
            with open(routing_file, "r") as f:
                print(f.read().strip())


# nrgf sub-command
def nrgf(args):
    cmd = [
        os.path.join(PIPELINE_DIR, "bin", f"runkcor_nrgf.sh"),
        args.flags,
        args.date[0],
        args.filename,
    ]

    process = launch_process(cmd, stdout=None, stderr=subprocess.PIPE)
    print("[%d] %s" % (process.pid, " ".join(cmd)))
    terminated = wait_for(process)


def print_help(args):
    args.parser.print_help()


if __name__ == "__main__":
    name = "KCor pipeline @GIT_VERSION@ [@GIT_REVISION@] (@GIT_BRANCH@)"

    parser = argparse.ArgumentParser(description=name)

    # top-level arguments
    parser.add_argument("-v", "--version", action="version", version=name)

    # show help if no sub-command given
    parser.set_defaults(func=print_help, parser=parser)

    subparsers = parser.add_subparsers(metavar="command")

    date_help = """dates to run on in the form YYYYMMDD including lists (using
                   commas) and ranges (using hyphens where end date is not
                   included)
                """
    flags_help = """FLAGS section of config filename, i.e., file in config/
                    directory matching kcor.FLAGS.cfg will be used"""

    # list sub-command
    list_parser = subparsers.add_parser(
        "list", help="list KCor processes running on local machine"
    )
    list_parser.set_defaults(func=list_processes, parser=list_parser)

    # versions sub-command
    versions_parser = subparsers.add_parser(
        "versions",
        aliases=["version"],
        help="list versions of processing code by observing date",
    )
    versions_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    versions_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    versions_parser.add_argument(
        "--filter", type=str, help="expression to filter versions by", default=None
    )
    versions_parser.add_argument(
        "-o",
        "--oldest",
        action="store_true",
        help="set to only display the oldest version",
    )
    versions_parser.add_argument(
        "-n",
        "--none",
        action="store_true",
        help="set to only display the dates with no version",
    )
    versions_parser.add_argument(
        "-s",
        "--summary",
        action="store_true",
        help="set to display a summary of versions",
    )
    versions_parser.set_defaults(func=versions, parser=versions_parser)

    # estimate sub-command
    estimate_parser = subparsers.add_parser(
        "estimate", help="estimate a completion time for reprocessing"
    )
    estimate_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    estimate_parser.add_argument(
        "--filter",
        type=str,
        help="expression to recognize completed versions",
        default=None,
    )
    estimate_parser.add_argument(
        "-s", "--start-date", help="specify the start date/time of the reprocessing"
    )
    estimate_parser.set_defaults(func=estimate, parser=estimate_parser)

    # report sub-command
    report_parser = subparsers.add_parser(
        "report", help="report basic facts on date(s)"
    )
    report_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    report_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    report_parser.add_argument(
        "-v", "--verbose", action="store_true", help="set to display verbose output"
    )
    report_parser.set_defaults(func=report, parser=report_parser)

    # ls sub-command
    ls_parser = subparsers.add_parser("ls", help="produce listing of KCor data files")
    ls_parser.add_argument(
        "files", nargs="*", default=".", help="KCor files(s)", metavar="file(s)"
    )
    ls_parser.add_argument(
        "-k", "--keywords", type=str, help="FITS keyword names to display", default=None
    )
    ls_parser.set_defaults(func=ls, parser=ls_parser)

    alert_parser = subparsers.add_parser(
        "alert", help="produce listing of JSON alerts in given directory"
    )
    alert_parser.add_argument(
        "--by-issue-time",
        action="store_true",
        help="sort JSON alerts by issue time instead of event time",
    )
    alert_parser.add_argument(
        "--detail", action="store_true", help="show full JSON for alerts"
    )
    alert_parser.add_argument(
        "json_files", nargs="*", help="JSON alert files", metavar="json_file"
    )
    alert_parser.set_defaults(func=list_alerts, parser=alert_parser)

    # log sub-command
    log_parser = subparsers.add_parser(
        "log", help="filter/display processing log output"
    )
    log_parser.add_argument(
        "logfiles", nargs="*", help="KCor log filename or date", metavar="logfile"
    )
    level_help = "filter level: DEBUG INFO WARN ERROR CRITICAL (default DEBUG)"
    log_parser.add_argument("-l", "--level", help=level_help)
    log_parser.add_argument(
        "-t", "--type", help="type of log: realtime, eod, cme", default="realtime"
    )
    log_parser.add_argument(
        "-p",
        "--prune",
        help="prune rotated logs with versions higher than MAX_VERSION",
        metavar="MAX_VERSION",
    )
    log_parser.add_argument(
        "-f", "--follow", help="output appended data as file grows", action="store_true"
    )
    log_parser.add_argument(
        "-d", "--debug", help="DEBUG filter level", action="store_true"
    )
    log_parser.add_argument(
        "-i", "--info", help="INFO filter level", action="store_true"
    )
    log_parser.add_argument(
        "-w", "--warn", help="WARN filter level", action="store_true"
    )
    log_parser.add_argument(
        "-e", "--error", help="ERROR filter level", action="store_true"
    )
    log_parser.add_argument(
        "-c", "--critical", help="CRITICAL filter level", action="store_true"
    )
    log_parser.set_defaults(func=filter_log, parser=log_parser)

    # missing sub-command
    missing_parser = subparsers.add_parser(
        "missing", help="list missing files for a given day"
    )
    missing_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    missing_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    missing_parser.add_argument(
        "-s", "--script", help="create bash script", action="store_true"
    )
    missing_parser.set_defaults(func=missing, parser=missing_parser)

    # process, eod, rt, cal sub-commands
    process_parser = subparsers.add_parser(
        "process", help="run realtime/end-of-day pipelines for given date(s)"
    )
    eod_parser = subparsers.add_parser(
        "end-of-day", aliases=["eod"], help="run end-of-day pipeline for given date(s)"
    )
    rt_parser = subparsers.add_parser(
        "realtime", aliases=["rt"], help="run realtime pipeline for given date(s)"
    )
    cal_parser = subparsers.add_parser(
        "calibration",
        aliases=["cal"],
        help="run calibration pipeline for given date(s)",
    )

    process_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    eod_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    rt_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    cal_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )

    process_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    eod_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    rt_parser.add_argument("-f", "--flags", type=str, help=flags_help, default="latest")
    cal_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )

    nowait_help = "set to run all dates simultaneously"
    process_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    eod_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    rt_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    cal_parser.add_argument("--no-wait", action="store_true", help=nowait_help)

    cal_parser.add_argument(
        "-l",
        "--list",
        type=str,
        metavar="LIST_FILENAME",
        help="set to a filename containing a list of files to use to produce the calibration",
    )

    process_parser.set_defaults(func=process, parser=process_parser)
    eod_parser.set_defaults(func=process_eod, parser=eod_parser)
    rt_parser.set_defaults(func=process_rt, parser=rt_parser)
    cal_parser.set_defaults(func=process_cal, parser=cal_parser)

    # script sub-command
    script_parser = subparsers.add_parser(
        "script", help="run a given script on given date(s)"
    )
    script_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    script_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    script_parser.add_argument(
        "-n", "--name", type=str, help="name of script", required=True
    )
    script_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    script_parser.set_defaults(func=run_script, parser=script_parser)

    # cme sub-command
    cme_parser = subparsers.add_parser(
        "cme", help="run CME detection in batch mode on given date"
    )
    cme_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    cme_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    cme_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    realtime_help = "set to use realtime setting instead of batch mode"
    cme_parser.add_argument("--realtime", action="store_true", help=realtime_help)
    cme_parser.set_defaults(func=cme_detection, parser=cme_parser)

    # savecme sub-command
    savecme_parser = subparsers.add_parser(
        "savecme", help="save CME results for given date(s)"
    )
    savecme_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    savecme_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    savecme_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    savecme_parser.set_defaults(func=savecme, parser=savecme_parser)

    # archive sub-command
    archive_parser = subparsers.add_parser(
        "archive", help="archive files for given date(s) to archival storage"
    )
    archive_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    archive_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    archive_parser.add_argument(
        "-l",
        "--level",
        type=str,
        help="level to archive, 0 or 1 (1.5), default=0",
        default="0",
    )
    archive_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    archive_parser.set_defaults(func=archive, parser=archive_parser)

    # verify sub-command
    verify_parser = subparsers.add_parser(
        "verify", help="verify previously processed date(s)"
    )
    verify_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    verify_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    verify_parser.set_defaults(func=verify, parser=verify_parser)

    # purge sub-command
    purge_parser = subparsers.add_parser(
        "purge", help="purge results from archive/database for given date(s)"
    )
    purge_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    purge_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    purge_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    purge_parser.set_defaults(func=purge, parser=purge_parser)

    # remove sub-command
    remove_help = "remove level1 directory for given date(s)"
    remove_parser = subparsers.add_parser("remove", help=remove_help)
    remove_parser.add_argument(
        "dates", type=str, nargs="*", help=date_help, metavar="date-expr"
    )
    remove_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    remove_parser.add_argument("--no-wait", action="store_true", help=nowait_help)
    remove_parser.set_defaults(func=remove, parser=remove_parser)

    # simulate sub-command
    simulate_parser = subparsers.add_parser(
        "simulate", aliases=["sim"], help="simulate realtime processing for given date"
    )
    simulate_parser.add_argument(
        "dates",
        type=str,
        nargs="*",
        help="date to run on in the form YYYYMMDD",
        metavar="date",
    )
    simulate_parser.add_argument(
        "--cme", action="store_true", help="run CME detection in realtime mode"
    )
    simulate_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    simulate_parser.add_argument(
        "--no-eod", action="store_true", help="set to not launch end-of-day processing"
    )
    simulate_parser.set_defaults(func=simulate, parser=simulate_parser)

    # stage sub-command
    stage_parser = subparsers.add_parser(
        "stage", help="stage raw data to a new location for the given date(s)"
    )
    stage_parser.add_argument(
        "dates",
        type=str,
        nargs="*",
        help="date to run on in the form YYYYMMDD",
        metavar="date",
    )
    stage_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="reprocess"
    )
    stage_parser.add_argument(
        "-l", "--location", type=str, help="location to copy raw data to", default="."
    )
    stage_parser.set_defaults(func=stage, parser=stage_parser)

    # locate sub-command
    locate_parser = subparsers.add_parser(
        "locate", aliases=["loc"], help="locate raw data for given date(s)"
    )
    locate_parser.add_argument(
        "dates",
        type=str,
        nargs="*",
        help="date to run on in the form YYYYMMDD",
        metavar="date",
    )
    locate_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="reprocess"
    )
    locate_parser.add_argument(
        "-v", "--verbose", action="store_true", help="set to display verbose output"
    )
    locate_parser.set_defaults(func=locate, parser=locate_parser)

    # nrgf sub-command
    nrgf_parser = subparsers.add_parser("nrgf", help="process an L0 file to a NRGF")
    nrgf_parser.add_argument(
        "date",
        type=str,
        nargs="*",
        help="date to run on in the form YYYYMMDD",
        metavar="date",
    )
    nrgf_parser.add_argument(
        "-f", "--flags", type=str, help=flags_help, default="latest"
    )
    nrgf_parser.add_argument("--filename", type=str, help="L0 basename to process")
    nrgf_parser.set_defaults(func=nrgf, parser=nrgf_parser)

    # parse args and call appropriate sub-command
    args = parser.parse_args()
    args.func(args)
